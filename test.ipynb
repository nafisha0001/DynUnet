{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb3b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.losses import DiceCELoss\n",
    "from torch.optim import Adam\n",
    "# from dataset import VSDataset\n",
    "from model import DynUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils.utils import load_nifti_as_dhw, dicom_load\n",
    "\n",
    "class VSDataset(Dataset):\n",
    "    def __init__(self, csv_path, data_dir, transform=None, target_slices=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.target_slices = target_slices\n",
    "        self.image_filenames = self.data['image_path'].tolist()\n",
    "        self.mask_filenames = self.data['SegmentationPath'].tolist()\n",
    "\n",
    "    def transform_volume(self, image_volume, mask_volume):\n",
    "        image_volume = image_volume.transpose(1, 2, 0)  \n",
    "        mask_volume = mask_volume.transpose(1, 2, 0)   \n",
    "        # print('before:-', image_volume.shape, mask_volume.shape)\n",
    "        transformed = self.transform(\n",
    "            image=image_volume,\n",
    "            mask=mask_volume\n",
    "        )\n",
    "        images = transformed['image']\n",
    "        masks = transformed['mask']\n",
    "        masks= masks.permute(2, 0, 1)\n",
    "        # print('after:- ',images.shape, masks.shape)\n",
    "        return images, masks.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.data_dir, self.mask_filenames[idx])\n",
    "\n",
    "        mask = load_nifti_as_dhw(mask_path)\n",
    "        image = dicom_load(image_path, mask.shape)\n",
    "        # print(image.shape, mask.shape)\n",
    "\n",
    "        transformed_image_volume, transformed_mask_volume = self.transform_volume(image, mask)\n",
    "        \n",
    "        # # Change to (C, D, H, W)\n",
    "        image_tensor = transformed_image_volume.unsqueeze(0)  \n",
    "        mask_tensor = transformed_mask_volume.unsqueeze(0)   \n",
    "        # print(image_tensor.shape, mask_tensor.shape)\n",
    "\n",
    "        # Now handle expansion or removal\n",
    "        current_slices = image_tensor.shape[1]\n",
    "\n",
    "        if self.target_slices:\n",
    "            if current_slices < self.target_slices:\n",
    "                # --- Duplicate labeled slices ---\n",
    "                # required_slices = self.target_slices - current_slices\n",
    "                labeled_slices = []\n",
    "                for i in range(current_slices):\n",
    "                    unique_vals = torch.unique(mask_tensor[0, i, :, :])\n",
    "                    if 0 in unique_vals and 1 in unique_vals:\n",
    "                        labeled_slices.append(i)\n",
    "\n",
    "                if len(labeled_slices) == 0:\n",
    "                    raise ValueError(\"No labeled slices (with both 0 and 1) found. Cannot duplicate.\")\n",
    "\n",
    "                while current_slices < self.target_slices:\n",
    "                    for i in labeled_slices:\n",
    "                        if current_slices < self.target_slices:\n",
    "                            image_tensor = torch.cat((image_tensor, image_tensor[:, i:i+1, :, :]), dim=1)\n",
    "                            mask_tensor = torch.cat((mask_tensor, mask_tensor[:, i:i+1, :, :]), dim=1)\n",
    "                            current_slices += 1\n",
    "\n",
    "            elif current_slices > self.target_slices:\n",
    "                # --- Remove unlabeled slices ---\n",
    "                unlabeled_slices = []\n",
    "                for i in range(current_slices):\n",
    "                    unique_vals = torch.unique(mask_tensor[0, i, :, :])\n",
    "                    if torch.all(unique_vals == 0):\n",
    "                        unlabeled_slices.append(i)\n",
    "\n",
    "                if len(unlabeled_slices) == 0:\n",
    "                    raise ValueError(\"No unlabeled slices found for removal.\")\n",
    "\n",
    "                slices_to_keep = list(range(current_slices))\n",
    "\n",
    "                # Remove unlabeled slices until reaching target\n",
    "                for i in unlabeled_slices:\n",
    "                    if len(slices_to_keep) > self.target_slices:\n",
    "                        slices_to_keep.remove(i)\n",
    "\n",
    "                # After removal, crop\n",
    "                image_tensor = image_tensor[:, slices_to_keep, :, :]\n",
    "                mask_tensor = mask_tensor[:, slices_to_keep, :, :]\n",
    "\n",
    "                # Final safety check\n",
    "                if image_tensor.shape[1] != self.target_slices:\n",
    "                    raise ValueError(f\"After removal, slice count {image_tensor.shape[1]} not equal to target {self.target_slices}.\")\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "        # return transformed_image_volume, transformed_mask_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6baf9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "LEARNING_RATE = 1e-4\n",
    "num_epochs = 4\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bcda7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_transform = A.Compose([\n",
    "    A.Resize(image_size, image_size),\n",
    "    A.Rotate(limit=35, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "    A.Normalize(mean=0.0, std=1.0, max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46464f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_path = 'D:\\\\3dVS1\\\\sample_data\\\\Masks'\n",
    "# images_path = 'D:\\\\3dVS1\\\\sample_data\\\\Image'\n",
    "data_dir= r'D:\\VSdata'\n",
    "# csv_path = r'D:\\VSdata\\vs_paths_cleaned.csv' \n",
    "csv_path= r\"C:\\Users\\Acer\\Desktop\\vs_paths.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2650e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VSDataset(\n",
    "    csv_path= csv_path,\n",
    "    data_dir=data_dir,\n",
    "    transform=slice_transform,  \n",
    "    target_slices=128            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c97ea0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128, 128]), torch.Size([1, 128, 128, 128]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = dataset[3]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a49c7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c8cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= DynUNet(spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=[3, 3, 3, 3, 3, 3],\n",
    "    strides=[1, 2, 2, 2, 2, 2],\n",
    "    upsample_kernel_size=[2, 2, 2, 2, 2],\n",
    "    res_block=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34032de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26fd32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8aa226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image: D:\\VSdata\\Vestibular-Schwannoma-MC-RC\\VS-MC-RC-048\\01-27-1995-NA-t4of5  MRI Head-13489\\15.000000-T2cor iams-56664\n",
      "Reading mask: D:\\VSdata\\VS-MC-RC segmentations-NIfTI-Release 2023/VS-MC-RC-048/1995-01-27/seg_T2.nii.gz\n",
      "[INFO] image_stack shape already matches target shape.\n",
      "[INFO] Final image_stack shape: (55, 640, 640)\n",
      "(55, 640, 640) (55, 640, 640)\n",
      "torch.Size([640, 128, 128]) torch.Size([128, 128, 640])\n",
      "Reading image: D:\\VSdata\\Vestibular-Schwannoma-MC-RC\\VS-MC-RC-015\\08-15-1990-NA-t2of3  External Images for PACS-98115\\301.000000-T2I.A.C.      T2 TSE-97536\n",
      "Reading mask: D:\\VSdata\\VS-MC-RC segmentations-NIfTI-Release 2023/VS-MC-RC-015/1990-08-15/seg_T2.nii.gz\n",
      "[INFO] image_stack shape already matches target shape.\n",
      "[INFO] Final image_stack shape: (30, 256, 256)\n",
      "(30, 256, 256) (30, 256, 256)\n",
      "torch.Size([256, 128, 128]) torch.Size([128, 128, 256])\n",
      "Reading image: D:\\VSdata\\Vestibular-Schwannoma-MC-RC\\VS-MC-RC-052\\09-26-1998-NA-t3of3  MRI IAMS-98521\\5.000000-T2FIESTA-C-94297\n",
      "Reading mask: D:\\VSdata\\VS-MC-RC segmentations-NIfTI-Release 2023/VS-MC-RC-052/1998-09-26/seg_T2.nii.gz\n",
      "[INFO] image_stack shape already matches target shape.\n",
      "[INFO] Final image_stack shape: (68, 512, 512)\n",
      "(68, 512, 512) (68, 512, 512)\n",
      "torch.Size([512, 128, 128]) torch.Size([128, 128, 512])\n",
      "Reading image: D:\\VSdata\\Vestibular-Schwannoma-MC-RC\\VS-SEG-032\\04-22-1996-NA-t4of7MRI IAMS-02675\\5.000000-T2t2spcrsttrap2iso-87095\n",
      "Reading mask: D:\\VSdata\\VS-MC-RC segmentations-NIfTI-Release 2023/VS-SEG-032/1996-04-22/seg_T2.nii.gz\n",
      "[INFO] image_stack shape already matches target shape.\n",
      "[INFO] Final image_stack shape: (72, 640, 640)\n",
      "(72, 640, 640) (72, 640, 640)\n",
      "torch.Size([640, 128, 128]) torch.Size([128, 128, 640])\n",
      "Reading image: D:\\VSdata\\Vestibular-Schwannoma-MC-RC\\VS-SEG-121\\01-29-1997-NA-t3of3MR IAMs-77655\\4.000000-T2t2spctrap2iso320-67813\n",
      "Reading mask: D:\\VSdata\\VS-MC-RC segmentations-NIfTI-Release 2023/VS-SEG-121/1997-01-29/seg_T2.nii.gz\n",
      "[INFO] image_stack shape already matches target shape.\n",
      "[INFO] Final image_stack shape: (72, 640, 640)\n",
      "(72, 640, 640) (72, 640, 640)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matrix_transform.cpp:784: error: (-215:Assertion failed) _src.dims() <= 2 in function 'cv::flip'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      6\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m, in \u001b[0;36mVSDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m image \u001b[38;5;241m=\u001b[39m dicom_load(image_path, mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape, mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 40\u001b[0m transformed_image_volume, transformed_mask_volume \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_volume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(transformed_image_volume\u001b[38;5;241m.\u001b[39mshape, transformed_mask_volume\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# slices, _, _ = image.shape\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# image_tensor = []\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# mask_tensor = []\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# return image_tensor, mask_tensor\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mVSDataset.transform_volume\u001b[1;34m(self, image_volume, mask_volume)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform_volume\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_volume, mask_volume):\n\u001b[1;32m---> 19\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_volume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_volume\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     images \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]                          \u001b[38;5;66;03m# shape: (D, H, W) or similar\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     masks \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]             \u001b[38;5;66;03m# add channel dimension: (1, D, H, W)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albumentations\\core\\composition.py:496\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 496\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_track_transform_params(t, data)\n\u001b[0;32m    498\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_data_post_transform(data)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albumentations\\core\\transforms_interface.py:176\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[1;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n\u001b[0;32m    175\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_with_params(params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albumentations\\core\\transforms_interface.py:200\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key2func \u001b[38;5;129;01mand\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key2func[key]\n\u001b[0;32m    199\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m ensure_contiguous_output(\n\u001b[1;32m--> 200\u001b[0m         target_function(ensure_contiguous_output(arg), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams),\n\u001b[0;32m    201\u001b[0m     )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m arg\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albumentations\\augmentations\\geometric\\transforms.py:1562\u001b[0m, in \u001b[0;36mHorizontalFlip.apply\u001b[1;34m(self, img, **params)\u001b[0m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albucore\\functions.py:717\u001b[0m, in \u001b[0;36mhflip\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhflip\u001b[39m(img: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhflip_cv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albucore\\decorators.py:42\u001b[0m, in \u001b[0;36mpreserve_channel_dim.<locals>.wrapped_function\u001b[1;34m(img, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_function\u001b[39m(img: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     41\u001b[0m     shape \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 42\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(img, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m NUM_MULTI_CHANNEL_DIMENSIONS \u001b[38;5;129;01mand\u001b[39;00m shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m MONO_CHANNEL_DIMENSIONS:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexpand_dims(result, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\3dVS\\lib\\site-packages\\albucore\\functions.py:713\u001b[0m, in \u001b[0;36mhflip_cv2\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;129m@preserve_channel_dim\u001b[39m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhflip_cv2\u001b[39m(img: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matrix_transform.cpp:784: error: (-215:Assertion failed) _src.dims() <= 2 in function 'cv::flip'\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        # outputs = model(inputs)\n",
    "        # outputs = torch.sigmoid(outputs)\n",
    "        # loss = loss_function(outputs, targets)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # running_loss += loss.item()\n",
    "        \n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    # avg_loss = running_loss / len(train_loader)\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d545d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>series_instance_uid</th>\n",
       "      <th>study_date</th>\n",
       "      <th>SegmentationPath</th>\n",
       "      <th>image_path</th>\n",
       "      <th>imagenifti_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>VS-MC-RC-036</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.2356822478735907152433...</td>\n",
       "      <td>07-21-1995</td>\n",
       "      <td>VS-MC-RC segmentations-NIfTI-Release 2023/VS-M...</td>\n",
       "      <td>manifest-1742405880893\\Vestibular-Schwannoma-M...</td>\n",
       "      <td>image-nifti\\VS-MC-RC-036\\07-21-1995\\image.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>VS-MC-RC-036</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.1160169313945234564236...</td>\n",
       "      <td>07-09-1999</td>\n",
       "      <td>VS-MC-RC segmentations-NIfTI-Release 2023/VS-M...</td>\n",
       "      <td>manifest-1742405880893\\Vestibular-Schwannoma-M...</td>\n",
       "      <td>image-nifti\\VS-MC-RC-036\\07-09-1999\\image.nii.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient_id                                series_instance_uid  \\\n",
       "182  VS-MC-RC-036  1.3.6.1.4.1.14519.5.2.1.2356822478735907152433...   \n",
       "183  VS-MC-RC-036  1.3.6.1.4.1.14519.5.2.1.1160169313945234564236...   \n",
       "\n",
       "     study_date                                   SegmentationPath  \\\n",
       "182  07-21-1995  VS-MC-RC segmentations-NIfTI-Release 2023/VS-M...   \n",
       "183  07-09-1999  VS-MC-RC segmentations-NIfTI-Release 2023/VS-M...   \n",
       "\n",
       "                                            image_path  \\\n",
       "182  manifest-1742405880893\\Vestibular-Schwannoma-M...   \n",
       "183  manifest-1742405880893\\Vestibular-Schwannoma-M...   \n",
       "\n",
       "                                      imagenifti_path  \n",
       "182  image-nifti\\VS-MC-RC-036\\07-21-1995\\image.nii.gz  \n",
       "183  image-nifti\\VS-MC-RC-036\\07-09-1999\\image.nii.gz  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = r'D:\\VSdata\\vs_paths.csv' \n",
    "import pandas as pd\n",
    "df= pd.read_csv(csv_path)\n",
    "df[df['patient_id']=='VS-MC-RC-036']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Transposing image_stack from (40, 512, 384) to (40, 384, 512) using order (0, 2, 1)\n",
      "[INFO] Final image_stack shape: (40, 384, 512)\n",
      "(40, 384, 512)\n",
      "(40, 384, 512)\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import load_nifti_as_dhw\n",
    "from utils.utils import dicom_load\n",
    "\n",
    "image= r'D:\\VSdata\\image-nifti\\VS-MC-RC-028\\10-12-1992\\image.nii.gz'\n",
    "mask = r'D:\\VSdata\\VS-MC-RC segmentations-NIfTI-Release 2023/VS-MC-RC-028/1992-10-12/seg_T2.nii.gz'\n",
    "path= r'D:\\VSdata\\Vestibular-Schwannoma-MC-RC\\VS-MC-RC-028\\10-12-1992-NA-t1of7  External Images for PACS-81342\\3.000000-T2t2 tra 3d-ciss-71390'\n",
    "\n",
    "mask= load_nifti_as_dhw(mask)\n",
    "image= dicom_load(path, mask.shape)\n",
    "\n",
    "print(image.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 384, 512)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "image= r'D:\\VSdata\\image-nifti\\VS-MC-RC-028\\10-12-1992\\image.nii.gz'\n",
    "image= load_nifti_as_dhw(image)\n",
    "print(image.shape)\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = r'D:\\VSdata\\vs_paths.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Remove 'manifest-<digits>\\\\' from the beginning of the image_path\n",
    "df['image_path'] = df['image_path'].str.replace(r'^manifest-\\d+\\\\', '', regex=True)\n",
    "\n",
    "# Save the cleaned dataframe back to CSV (overwrite original)\n",
    "df.to_csv(r'D:\\VSdata\\vs_paths_cleaned.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dVS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
